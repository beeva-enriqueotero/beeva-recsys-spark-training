{"paragraphs":[{"text":"%md\n\n# Recommender System with Spark MLlib. \n## Example 2: Item similarity\n### Load dataset","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694170550_1281037180","id":"20161017-104316_1857293220","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Recommender System with Spark MLlib.</h1>\n<h2>Example 2: Item similarity</h2>\n<h3>Load dataset</h3>\n"},"dateCreated":"2016-10-17T10:49:30+0200","dateStarted":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:34:37+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4915","focus":true},{"text":"import org.apache.spark.mllib.evaluation.{RankingMetrics}\nimport org.apache.spark.mllib.recommendation.{ALS, Rating}\n\nimport org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors._\nimport org.apache.spark.mllib.linalg.SparseVector\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.linalg.distributed.MatrixEntry\n\n\nval PATH = \"/home/enriqueotero/datasets/movielens/ml-100k/\"\nval TRAINFILE = \"u1.base\"\nval TESTFILE = \"u1.test\"\n\n  // Read in the ratings data\n  val ratings = sc.textFile(PATH + TRAINFILE).map { line =>\n    val fields = line.split(\"\\t\")\n    Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble - 2.5)\n  }.cache()\n\n\n  val ratingstest = sc.textFile(PATH + TESTFILE).map { line =>\n    val fields = line.split(\"\\t\")\n    Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble - 2.5)\n  }.cache()\n\n  // Map ratings to 1 or 0, 1 indicating a movie that should be recommended\n  val binarizedRatings = ratingstest.map(r => Rating(r.user, r.product,\n    if (r.rating > 0) 1.0 else 0.0)).cache()","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694170550_1281421929","id":"20161017-103954_976150493","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.mllib.evaluation.RankingMetrics\n\nimport org.apache.spark.mllib.recommendation.{ALS, Rating}\n\nimport org.apache.spark.mllib.linalg.Vector\n\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.mllib.linalg.Vectors._\n\nimport org.apache.spark.mllib.linalg.SparseVector\n\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\n\nimport org.apache.spark.mllib.linalg.distributed.MatrixEntry\n\nPATH: String = /home/enriqueotero/datasets/movielens/ml-100k/\n\nTRAINFILE: String = u1.base\n\nTESTFILE: String = u1.test\n\nratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[850] at map at <console>:72\n\nratingstest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[853] at map at <console>:72\n\nbinarizedRatings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[854] at map at <console>:114\n"},"dateCreated":"2016-10-17T10:49:30+0200","dateStarted":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:34:39+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4916","focus":true},{"text":"%md\n### Analyze dataset","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694253006_1684393589","id":"20161017-105053_816614981","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Analyze dataset</h3>\n"},"dateCreated":"2016-10-17T10:50:53+0200","dateStarted":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:34:37+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4917"},{"text":"\n// Summarize ratings\n  val numRatings = ratings.count()\n  val numUsers = ratings.map(_.user).distinct().count()\n  val numMovies = ratings.map(_.product).distinct().count()\n  println(s\"Got $numRatings ratings from $numUsers users on $numMovies movies.\")\n  \n\nval itemsAll = ratings.map(r => (r.product,r.user))\nval itemsFiltered = ratings.filter(_.rating>0).map(r => (r.product,r.user))\nval numItems = itemsFiltered.keys.distinct().count()\nprintln(\"num items : \"+numItems)\n\nval maxItem = itemsFiltered.keys.max() + 1\n\nval users = itemsAll.map{case (item,user) => (user,item)}","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694545767_-960356671","id":"20161017-105545_1132278174","result":{"code":"SUCCESS","type":"TEXT","msg":"\nnumRatings: Long = 80000\n\nnumUsers: Long = 943\n\nnumMovies: Long = 1650\nGot 80000 ratings from 943 users on 1650 movies.\n\nitemsAll: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[863] at map at <console>:114\n\nitemsFiltered: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[865] at map at <console>:114\n\nnumItems: Long = 1548\nnum items : 1548\n\nmaxItem: Int = 1683\n\nusers: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[871] at map at <console>:116\n"},"dateCreated":"2016-10-17T10:55:45+0200","dateStarted":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:34:40+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4918","focus":true},{"text":"%md\n### Build the model","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694596699_1199901134","id":"20161017-105636_1951042939","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Build the model</h3>\n"},"dateCreated":"2016-10-17T10:56:36+0200","dateStarted":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:34:37+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4919"},{"text":"  // Build the model\n\ndef getUserVectors(rdd : org.apache.spark.rdd.RDD[(Int,Int)],minItemsPerUser : Int,maxItem :Int) : org.apache.spark.rdd.RDD[Vector] =\n  {\n    rdd.groupByKey().filter(_._2.size >= minItemsPerUser)\n     .map{ case (user,items) =>\n      Vectors.sparse(maxItem, items.map(item => (item,1.toDouble)).toSeq)\n      }\n  }\n\nval begin = System.currentTimeMillis\nval userVectors = getUserVectors(users, 1, maxItem)\n\nval numUsers = userVectors.count()\nprintln(\"Number of users : \"+numUsers)\n\nval r = new RowMatrix(userVectors);\n\nval THRESHOLD = 0.1\nprintln(\"Running item similarity with threshold :\"+THRESHOLD)\ndef runDimSum(r :RowMatrix,dimsumThreshold : Double) : org.apache.spark.rdd.RDD[MatrixEntry] =\n{\n  r.columnSimilarities(dimsumThreshold).entries\n}\n\nval simItems = runDimSum(r, THRESHOLD)\n\n/* Get n=LIMIT neighbors per item */\n\ndef sortAndLimit(similarities : org.apache.spark.rdd.RDD[MatrixEntry],limit : Int) = {\n    val v = similarities.map{me => (me.i,(me.j,me.value))}.groupByKey().mapValues(_.toSeq.sortBy{ case (domain, count) => count }(Ordering[Double].reverse).take(limit)).flatMapValues(v => v)\n    v\n  }\n\nval LIMIT = 100\nval mysims = sortAndLimit(simItems,LIMIT)\nval end = System.currentTimeMillis\nprintln(\"Elapsed time for model \" + (end-begin)/1000.0 + \"s\")\n","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694667937_1399075182","id":"20161017-105747_2039026102","result":{"code":"SUCCESS","type":"TEXT","msg":"\ngetUserVectors: (rdd: org.apache.spark.rdd.RDD[(Int, Int)], minItemsPerUser: Int, maxItem: Int)org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector]\n\nbegin: Long = 1476700481057\n\nuserVectors: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[874] at map at <console>:123\n\nnumUsers: Long = 943\nNumber of users : 943\n\nr: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@6e8040ab\n\nTHRESHOLD: Double = 0.1\nRunning item similarity with threshold :0.1\n\nrunDimSum: (r: org.apache.spark.mllib.linalg.distributed.RowMatrix, dimsumThreshold: Double)org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.MatrixEntry]\nElapsed time for model 1.136s\n\n\n\n\n\nsimItems: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.MatrixEntry] = MapPartitionsRDD[878] at map at RowMatrix.scala:656\nsortAndLimit: (similarities: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.MatrixEntry], limit: Int)org.apache.spark.rdd.RDD[(Long, (Long, Double))]\nLIMIT: Int = 100\nmysims: org.apache.spark.rdd.RDD[(Long, (Long, Double))] = MapPartitionsRDD[882] at flatMapValues at <console>:149\nend: Long = 1476700482193\n"},"dateCreated":"2016-10-17T10:57:47+0200","dateStarted":"2016-10-17T12:34:39+0200","dateFinished":"2016-10-17T12:34:42+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4920","focus":true},{"text":"%md\n### Generate recommendations","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694676294_-1716236662","id":"20161017-105756_716043208","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Generate recommendations</h3>\n"},"dateCreated":"2016-10-17T10:57:56+0200","dateStarted":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:34:37+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4921"},{"text":"/* Generate K recomendations for every user*/\n\nval K = 10\nval NUM_RECS = 10\nval mysims2 = mysims.map{case (id1,(id2,score)) => (id1.toInt, (id2, score))}\nval NUM_INTERACTIONS = 50\n\nval itemsFiltered2 = ratings.filter(_.rating>=0.0).map(r => (r.user,r.product)).groupByKey().mapValues(_.take(NUM_INTERACTIONS)).flatMapValues(v=>v).map{case(u,i)=>(i,u)}\n//val itemsFiltered2 = ratings.filter(_.rating>=0.0).map(r => (r.user,(r.product,r.rating))).groupByKey().mapValues(_.toArray.sortBy{ case (domain, count) => count }(Ordering[Double].reverse)).flatMapValues(v=>v).map{case(u,(i,r))=>(i,u)}\n//val itemsFiltered2 = ratings.filter(_.rating>=0.0).map(r => (r.user,(r.product,r.rating))).groupByKey().mapValues(_.toArray.sortBy{ case (domain, count) => count }(Ordering[Double])).flatMapValues(v=>v).map{case(u,(i,r))=>(i,u)}\nval myjoin=mysims2.join(itemsFiltered2)\nval myjoin2 = myjoin.map{case (id1, ((id2, score), user)) => ((user, id2), score)}\nval myjoin3 = myjoin2.reduceByKey(_+_)\nval myjoin4 = myjoin3.map{case ((user,id2),score) => (user,(id2,score))}\nval myrecs = myjoin4.groupByKey().mapValues(_.toArray.sortBy{ case (domain, count) => count }(Ordering[Double].reverse).take(NUM_RECS)).flatMapValues(v => v)\n\nval myrecs2 = myrecs.map{case (user,(item,score)) => (user,Rating(user,item.toInt,score))}.groupByKey()\nval userRecommended = myrecs2.map{case (user, iterable) => (user,iterable.toArray)}\n\n// Remove from recommendations items in training set\nval itemsFiltered3 = itemsFiltered2.map(x=>(x._2,x._1))\nval userRecommended2 = userRecommended.cogroup(itemsFiltered3)\nval userRecommended3 = userRecommended2.map(x=>(x._1, (x._2._1.flatMap(r=>r).filter(r=> !x._2._2.toArray.contains(r.product))).toArray))","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694725585_-206589247","id":"20161017-105845_1084813655","result":{"code":"SUCCESS","type":"TEXT","msg":"\nK: Int = 10\n\nNUM_RECS: Int = 10\n\nmysims2: org.apache.spark.rdd.RDD[(Int, (Long, Double))] = MapPartitionsRDD[883] at map at <console>:137\n\nNUM_INTERACTIONS: Int = 50\n\nitemsFiltered2: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[889] at map at <console>:130\n\nmyjoin: org.apache.spark.rdd.RDD[(Int, ((Long, Double), Int))] = MapPartitionsRDD[892] at join at <console>:143\n\nmyjoin2: org.apache.spark.rdd.RDD[((Int, Long), Double)] = MapPartitionsRDD[893] at map at <console>:145\n\nmyjoin3: org.apache.spark.rdd.RDD[((Int, Long), Double)] = ShuffledRDD[894] at reduceByKey at <console>:147\n\nmyjoin4: org.apache.spark.rdd.RDD[(Int, (Long, Double))] = MapPartitionsRDD[895] at map at <console>:149\n\nmyrecs: org.apache.spark.rdd.RDD[(Int, (Long, Double))] = MapPartitionsRDD[898] at flatMapValues at <console>:153\n\nmyrecs2: org.apache.spark.rdd.RDD[(Int, Iterable[org.apache.spark.mllib.recommendation.Rating])] = ShuffledRDD[900] at groupByKey at <console>:155\n\nuserRecommended: org.apache.spark.rdd.RDD[(Int, Array[org.apache.spark.mllib.recommendation.Rating])] = MapPartitionsRDD[901] at map at <console>:157\n\nitemsFiltered3: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[902] at map at <console>:132\n\nuserRecommended2: org.apache.spark.rdd.RDD[(Int, (Iterable[Array[org.apache.spark.mllib.recommendation.Rating]], Iterable[Int]))] = MapPartitionsRDD[904] at cogroup at <console>:161\n\nuserRecommended3: org.apache.spark.rdd.RDD[(Int, Array[org.apache.spark.mllib.recommendation.Rating])] = MapPartitionsRDD[905] at map at <console>:163\n"},"dateCreated":"2016-10-17T10:58:45+0200","dateStarted":"2016-10-17T12:34:40+0200","dateFinished":"2016-10-17T12:34:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4922","focus":true},{"text":"%md\n### Eval recommendations","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694771873_563955530","id":"20161017-105931_1582543689","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Eval recommendations</h3>\n"},"dateCreated":"2016-10-17T10:59:31+0200","dateStarted":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:34:37+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4923"},{"text":"// Assume that any movie a user rated 3 or higher (which maps to a 1) is a relevant document\n// Compare with top K recommendations\nval userTest = binarizedRatings.groupBy(_.user)\nval userJoin = userTest.join(userRecommended3)\n\nvar relevantDocuments = userJoin.map { case (user, (actual, predictions)) =>\n  (predictions.map(_.product), actual.filter(_.rating > 0.0).map(_.product).toArray)\n}\n\n// Skip users with no data in test\nrelevantDocuments = relevantDocuments.filter(_._2.length>0)\n\n\n// Instantiate metrics object\nval metrics = new RankingMetrics(relevantDocuments)\n\nval begin = System.currentTimeMillis\n// Mean average precision\nprintln(s\"Mean average precision = ${metrics.meanAveragePrecision}\")\nval end = System.currentTimeMillis\nprintln(\"Elapsed time for MAP \" + (end-begin)/1000.0 + \"s\")\n\nlazy val meanAveragePrecisionAtK: Double = {\n    relevantDocuments.map { case (pred, lab) =>\n      val labSet = lab.toSet\n\n      if (labSet.nonEmpty) {\n        var i = 0\n        var cnt = 0\n        var precSum = 0.0\n\n        var pred2 = pred.slice(0,K)\n        val n = pred2.length\n\n        while (i < n) {\n          if (labSet.contains(pred2(i))) {\n            cnt += 1\n            precSum += cnt.toDouble / (i + 1)\n          }\n          i += 1\n        }\n\n        0.0+precSum / math.min(labSet.size,K)\n      } else {\n        println(\"Empty ground truth set, check input data\")\n        0.0\n      }\n    }.mean()\n  }\n\nprintln(s\"MAP@$K = ${meanAveragePrecisionAtK}\")","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694844184_232225358","id":"20161017-110044_1780113956","result":{"code":"SUCCESS","type":"TEXT","msg":"\nuserTest: org.apache.spark.rdd.RDD[(Int, Iterable[org.apache.spark.mllib.recommendation.Rating])] = ShuffledRDD[907] at groupBy at <console>:118\n\nuserJoin: org.apache.spark.rdd.RDD[(Int, (Iterable[org.apache.spark.mllib.recommendation.Rating], Array[org.apache.spark.mllib.recommendation.Rating]))] = MapPartitionsRDD[910] at join at <console>:173\n\nrelevantDocuments: org.apache.spark.rdd.RDD[(Array[Int], Array[Int])] = MapPartitionsRDD[911] at map at <console>:175\n\nrelevantDocuments: org.apache.spark.rdd.RDD[(Array[Int], Array[Int])] = MapPartitionsRDD[912] at filter at <console>:177\n\nmetrics: org.apache.spark.mllib.evaluation.RankingMetrics[Int] = org.apache.spark.mllib.evaluation.RankingMetrics@2d4bfbc4\n\nbegin: Long = 1476700486215\nMean average precision = 0.03290072963685718\n\nend: Long = 1476700503320\nElapsed time for MAP 17.105s\n\nmeanAveragePrecisionAtK: Double = <lazy>\nMAP@10 = 0.08491631940842184\n"},"dateCreated":"2016-10-17T11:00:44+0200","dateStarted":"2016-10-17T12:34:42+0200","dateFinished":"2016-10-17T12:35:04+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4924","focus":true},{"text":"%md\n### Example","dateUpdated":"2016-10-17T12:34:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476694850432_-1652674828","id":"20161017-110050_1954011114","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Example</h3>\n"},"dateCreated":"2016-10-17T11:00:50+0200","dateStarted":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:34:37+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4925","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476700018842_521592268","id":"20161017-122658_1804938808","dateCreated":"2016-10-17T12:26:58+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5845","dateUpdated":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:35:05+0200","dateStarted":"2016-10-17T12:34:45+0200","result":{"code":"SUCCESS","type":"TEXT","msg":"\ntotalRecs: Long = 9430\n\nmovies: org.apache.spark.rdd.RDD[String] = /home/enriqueotero/datasets/movielens/ml-100k/u.item MapPartitionsRDD[919] at textFile at <console>:70\ntitles: scala.collection.Map[Int,String] = Map(137 -> Big Night (1996), 891 -> Bent (1997), 550 -> Die Hard: With a Vengeance (1995), 1205 -> Secret Agent, The (1996), 146 -> Unhook the Stars (1996), 864 -> My Fellow Americans (1996), 559 -> Interview with the Vampire (1994), 218 -> Cape Fear (1991), 568 -> Speed (1994), 227 -> Star Trek VI: The Undiscovered Country (1991), 765 -> Boomerang (1992), 1115 -> Twelfth Night (1996), 774 -> Prophecy, The (1995), 433 -> Heathers (1989), 92 -> True Romance (1993), 1528 -> Nowhere (1997), 846 -> To Gillian on Her 37th Birthday (1996), 1187 -> Switchblade Sisters (1975), 1501 -> Prisoner of the Mountains (Kavkazsky Plennik) (1996), 442 -> Amityville Curse, The (1990), 1160 -> Love! Valour! Compassion! (1997), 101 -> Heavy Metal (1981), 1196 -> Sa...\nmyTitle: String = Frighteners, The (1996)\n\nmyUserFavorites: Seq[String] = ArrayBuffer(Star Wars (1977), Liar Liar (1997), When We Were Kings (1996), Tetsuo II: Body Hammer (1992))\n\nmyUserRecs: Seq[(String, Double)] = ArrayBuffer((Return of the Jedi (1983),0.7810805062703031), (Raiders of the Lost Ark (1981),0.6567996186027633), (Empire Strikes Back, The (1980),0.6457376926202624), (Independence Day (ID4) (1996),0.631437650466468), (Godfather, The (1972),0.6286053039063337), (Fargo (1996),0.6129337920651653), (Back to the Future (1985),0.6063039660181642), (Indiana Jones and the Last Crusade (1989),0.6046955722342844), (Star Trek: First Contact (1996),0.6017685839954346), (Silence of the Lambs, The (1991),0.600969143838293))\n"},"text":"val totalRecs = userRecommended.flatMapValues(x=>x).count()\n\nval movies = sc.textFile(PATH + \"u.item\")\nval titles = movies.map(line => line.split(\"\\\\|\").take(2)).map(array => (array(0).toInt,  array(1))).collectAsMap()\n\nval myTitle=titles(123)\nval myUserFavorites=users.lookup(310).map(titles)\nval myUserRecs=userRecommended.lookup(310).flatMap(r=>r).map(r=>(titles(r.product), r.rating))"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476700058248_950263839","id":"20161017-122738_183218966","dateCreated":"2016-10-17T12:27:38+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5926","dateUpdated":"2016-10-17T12:34:37+0200","dateFinished":"2016-10-17T12:35:05+0200","dateStarted":"2016-10-17T12:35:04+0200","result":{"code":"SUCCESS","type":"TEXT","msg":""},"text":""}],"name":"RecSys with Spark MLlib: Example 2","id":"2BZVGC9EV","angularObjects":{"2C1SMUNR2:shared_process":[],"2BZKD1JZ1:shared_process":[],"2BZDY9XYZ:shared_process":[],"2BZAHUAX3:shared_process":[],"2BYA9VV3D:shared_process":[],"2BZXVZE6D:shared_process":[],"2C1KZHX69:shared_process":[],"2BZXKCJRX:shared_process":[],"2BYNPN8SK:shared_process":[],"2BYBJCDXJ:shared_process":[],"2C1AMSD91:shared_process":[],"2BZR97E3N:shared_process":[],"2C15P4D8G:shared_process":[],"2C1RNCA8C:shared_process":[],"2BXA1FW3F:shared_process":[],"2BXSN6C59:shared_process":[],"2C15PG1TG:shared_process":[],"2C21UNK62:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}